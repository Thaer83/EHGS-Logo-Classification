{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyPHS/lPeu4w/MK4fD/zs+vk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MU2nmx_uHeJX"},"outputs":[],"source":["# main.py\n","#from data_preparation import load_and_preprocess_image\n","from image_processing import load_and_preprocess_image, load_and_preprocess_image_without_selective\n","from model_utils import save_history_to_excel, plot_convergence_curves, plot_confusion_matrix\n","#from optimizers.HGS_optimizer import optimize_hyperparameters\n","from optimizers.HGS_optimizer import HGS as hgs\n","#from models.vgg16_model import construct_model\n","#from trainers.model_trainer import train_model, test_model\n","\n","\n","\n","from keras.applications.vgg16 import preprocess_input, decode_predictions\n","\n","from sklearn.model_selection import train_test_split\n","\n","#import the OpenCV library in Python, specifically the cv2 module, The cv2 module contains functions for image processing and computer vision tasks.\n","import os, cv2, keras\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.utils import to_categorical #Thaer\n","\n","from keras.applications.vgg16 import VGG16\n","# from PIL import Image\n","import matplotlib.pyplot as plt #thaer\n","import pandas as pd #thaer\n","from google.colab import drive\n","import sys\n","import pickle as pkl\n","sys.path.append('/')\n","\n","#import selector as slctr\n","import pandas as pd\n","import fitnessFUN\n","import time\n","\n","\n","# Select number of repetitions for each experiment.\n","NumOfRuns=1\n","\n","# Select general parameters for all optimizers (population size, number of iterations)\n","PopulationSize = 5\n","Iterations= 10\n","\n","#Export results ?\n","Export=True\n","\n","#Automaticly generated file name by date and time\n","ExportToFile=\"experiment\"+time.strftime(\"%Y-%m-%d-%H-%M-%S\")+\".csv\"\n","\n","# Check if it works at least once\n","Flag=False\n","\n","# CSV Header for for the convergence\n","CnvgHeader1=[]\n","CnvgHeader2=[]\n","\n","\n","for l in range(0,Iterations):\n","\tCnvgHeader1.append(\"Iter\"+str(l+1))\n","\n","for l in range(0,Iterations):\n","\tCnvgHeader2.append(\"Iter\"+str(l+1))\n","\n","#++++++++++++++++++++++++++++++++++++  Data loading and preprocessing +++++++++++++++++++++++++++++++++\n","annot= \"flickr_logos_27_dataset_original/flickr_logos_27_dataset_training_set_annotation.txt\"\n","path = \"flickr_logos_27_dataset_original/flickr_logos_27_dataset_images\"\n","base_path = \"flickr_logos_27_dataset_original/\"\n","#/content/\n","# img = cv2.imread('images/test.jpeg')\n","cv2.setUseOptimized(True)   #tells OpenCV to use optimized code whenever possible, which can lead to better performance for many operations\n","cv2.setNumThreads(8)  # This function sets the number of threads that OpenCV will use for parallel operations\n","\n","#train_images, train_labels, train_labelsnum, train_labelsnumSet = load_and_preprocess_image(path, annot)\n","train_images, train_labels, train_labelsnum, train_labelsnumSet = load_and_preprocess_image_without_selective(path, annot)\n","\n","yarr = np.array(train_labelsnum)\n","\n","for k in range (0,NumOfRuns):\n","    # Split data into training and testing\n","    train_x, test_x, train_y, test_y = train_test_split(train_images, yarr, test_size=0.25)\n","    # Convert labels to categorical\n","    train_labels = to_categorical(train_y, num_classes=len(train_labelsnumSet))\n","    # test_labels = to_categorical(test_y, num_classes=8)\n","\n","    y_train = to_categorical(train_y, len(train_labelsnumSet))\n","    y_test = to_categorical(test_y, len(train_labelsnumSet))\n","\n","    train_ds = train_x\n","    test_ds = test_x\n","\n","    #============ checkpoint =====================\n","    # Assuming train_images is a list of numpy arrays\n","    train_ds = tf.convert_to_tensor(train_ds)\n","    test_ds = tf.convert_to_tensor(test_ds)\n","    print(\"the length is : \", len(train_labelsnumSet))\n","    print(\"the data is : \", train_labelsnumSet)\n","\n","    #to save it\n","    with open(\"lbldata.txt\", 'w') as f:\n","        for s in train_labelsnumSet:\n","            f.write(str(s) + '\\n')\n","\n","    # Define lower and upper bounds for each hyperparameter\n","    lb = [0.0001, 128, 0, 0, 3, 16]  # Lower bounds\n","    ub = [0.01, 4096, 2, 2, 10, 128] # Upper bounds   or  ub = [0.01, 4096, 2, 2, 10, 128]\n","\n","    dim = len(lb)\n","    function_name = \"objective_function\"\n","    # Call HGS\n","\n","    x=hgs(getattr(fitnessFUN, function_name), lb, ub, dim, PopulationSize, Iterations, args=(train_ds, train_labels, train_labelsnumSet))\n","\n","    if(Export==True):\n","        with open(ExportToFile, 'a',newline='\\n') as out:\n","            writer = csv.writer(out,delimiter=',')\n","            if (Flag==False): # just one time to write the header of the CSV file\n","                header= numpy.concatenate([[\"Optimizer\",\"Dataset\",\"objfname\",\"Experiment\",\"startTime\",\"EndTime\",\"ExecutionTime\",\"trainAcc\",\"testAcc\"],CnvgHeader1,CnvgHeader1])\n","                writer.writerow(header)\n","            a=numpy.concatenate([['HGS',\"Flicker27\",x.objfname,k+1,x.startTime,x.endTime,x.executionTime,\"50\",\"50\"],x.convergence1,x.convergence2])\n","            writer.writerow(a)\n","        out.close()\n","    Flag=True # at least one experiment\n","\n","if (Flag==False): # Faild to run at least one experiment\n","    print(\"No Optomizer or Cost function is selected. Check lists of available optimizers and cost functions\")\n","\n","    # Optimize hyperparameters (This is where PSO optimization would be invoked)\n","    #optimized_hyperparameters = optimize_hyperparameters(train_images, train_labels, train_labelsnumSet)\n","\n","#def main():\n","    # Data loading and preprocessing ++++++++++++++++++++++++++++++\n","    #path = \"flickr_logos_27_dataset_original/flickr_logos_27_dataset_images\"\n","    #annot = \"flickr_logos_27_dataset_original/flickr_logos_27_dataset_training_set_annotation.txt\"\n","    #train_images, train_labels, train_labelsnum, train_labelsnumSet = load_and_preprocess_image(path, annot)\n","\n","    # Convert labels to categorical ++++++++++++++++++++++++++++++++\n","    #train_labels = np.array(train_labelsnum)\n","    #train_labels = to_categorical(train_labels, num_classes=len(train_labelsnumSet))\n","\n","    # Split data into training and testing ++++++++++++++++++++++++\n","    # Assuming you have a utility function for this or use train_test_split as before\n","    # train_x, test_x, train_y, test_y = ...\n","\n","    # Optimize hyperparameters (This is where PSO optimization would be invoked) +++++++++++++++++\n","    #optimized_hyperparameters = optimize_hyperparameters(train_images, train_labels, train_labelsnumSet)\n","\n","    # Construct, train, and evaluate the model using the optimized hyperparameters\n","    # model = construct_model(...)\n","    # history = train_model(...)\n","    # test_model(...)\n","\n","#if __name__ == \"__main__\":\n","#    main()\n"]}]}